{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly\n",
    "# import plotly.graph_objs.Scatter as go\n",
    "from plotly.offline import init_notebook_mode, plot, iplot, download_plotlyjs\n",
    "from plotly.graph_objs import Scatter, Figure, Layout\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import pandas as pd\n",
    "import tables  # This will fail if you don't have 'pytables' installed to read the cache file\n",
    "\n",
    "import dask.dataframe as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data'\n",
    "RAW_DATA_DIR = DATA_DIR + '/raw'\n",
    "LOOKUPS = RAW_DATA_DIR + \"/Lookups\"\n",
    "TRANSACT = RAW_DATA_DIR + \"/Transactions\"\n",
    "\n",
    "SUBSET_FILE = DATA_DIR + '/' + 'subset.hdf'\n",
    "THINNED_FILE = DATA_DIR + '/' + 'thinned.hdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir(TRANSACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stores = pd.read_csv(LOOKUPS + '/stores.txt', sep='\\t')\n",
    "drugs = pd.read_csv(LOOKUPS + '/Drug_LookUp.txt', sep='\\t')\n",
    "illness = pd.read_csv(LOOKUPS + '/ChronicIllness_LookUp.txt', sep='\\t')\n",
    "patients = pd.read_csv(LOOKUPS + '/patients.txt', sep='\\t')\n",
    "atc = pd.read_csv(LOOKUPS + '/ATC_LookUp.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = illness.merge(drugs, on='MasterProductID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subset(frac=0.1):\n",
    "    samples = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(TRANSACT + '/' + file, sep='\\t')\n",
    "        sample = df.sample(frac=frac)\n",
    "        samples.append(sample)\n",
    "\n",
    "    return pd.concat(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_subset(subset):\n",
    "    subset = subset.merge(t1, left_on='Drug_ID', right_on='MasterProductID', suffixes=('_illness', '_drug'), how='outer')    \n",
    "    subset = subset.merge(patients, on='Patient_ID', how='outer')\n",
    "    subset['target'] = subset['ChronicIllness'] == 'Diabetes'\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subset_cached():\n",
    "    \n",
    "    if os.path.exists(SUBSET_FILE):\n",
    "        print(\"Using cached file\")\n",
    "        return pd.read_hdf(SUBSET_FILE, '/data')\n",
    "    \n",
    "    subset = get_subset()\n",
    "    subset = expand_subset(subset)\n",
    "    \n",
    "    subset.to_hdf(SUBSET_FILE, '/data')\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_thinned_cached(subset=None):\n",
    "    \n",
    "    if os.path.exists(THINNED_FILE):\n",
    "        return pd.read_hdf(THINNED_FILE, '/data')\n",
    "    \n",
    "    if subset is None:\n",
    "        subset = get_subset_cached()\n",
    "        \n",
    "    relevant = [\n",
    "        'Patient_ID',\n",
    "        'Store_ID',\n",
    "        'Prescriber_ID',\n",
    "        'Drug_ID',\n",
    "        'Prescription_Week',\n",
    "        'Dispense_Week',\n",
    "        'Drug_Code',\n",
    "        'NHS_Code',\n",
    "        'PatientPrice_Amt', \n",
    "        'WholeSalePrice_Amt',\n",
    "        'GovernmentReclaim_Amt', \n",
    "        'StreamlinedApproval_Code', \n",
    "        'ChemistListPrice',\n",
    "        'gender', \n",
    "        'year_of_birth', \n",
    "        'postcode', \n",
    "        'target'                \n",
    "    ]\n",
    "    \n",
    "    thinned = subset[relevant]\n",
    "    \n",
    "    thinned.to_hdf(THINNED_FILE, '/data')\n",
    "    return thinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thinned = get_thinned_cached()\n",
    "include_fields = ['Patient_ID', 'year_of_birth', 'postcode', 'gender', 'target']\n",
    "train_fields = ['Patient_ID', 'year_of_birth', 'postcode', 'gender']\n",
    "train = thinned[include_fields].copy()\n",
    "train['gender'] = train['gender'] == 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986955865277415\n"
     ]
    }
   ],
   "source": [
    "valid_ratio = len(train.dropna()) / len(train)\n",
    "print(str(valid_ratio))\n",
    "\n",
    "if valid_ratio > .95:\n",
    "    train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=8, input_dim=4))  # 4 input values connect to 16 input nodes\n",
    "model.add(Activation('relu'))  # Relu the suckers\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=16))  \n",
    "model.add(Activation('softmax'))\n",
    "model.add(Dense(units=1))  \n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train_fields].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_year = train['year_of_birth'].max()\n",
    "train['year_of_birth'] = train['year_of_birth'] / max_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5959649/5959649 [==============================] - 610s - loss: 0.9395 - acc: 0.0605   \n",
      "Epoch 2/5\n",
      "5959649/5959649 [==============================] - 2132s - loss: 0.9395 - acc: 0.0605  \n",
      "Epoch 3/5\n",
      "5959649/5959649 [==============================] - ETA: 0s - loss: 0.9395 - acc: 0.060 - 545s - loss: 0.9395 - acc: 0.0605   \n",
      "Epoch 4/5\n",
      "2835904/5959649 [=============>................] - ETA: 273s - loss: 0.9394 - acc: 0.0606"
     ]
    }
   ],
   "source": [
    "model.fit(train[train_fields].as_matrix(),  # X / inputs\n",
    "          train['target'].as_matrix(),      # y / targets\n",
    "          epochs=5,             # number of training epochs\n",
    "          batch_size=32)        # batch size (number-at-once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(rf, open(\"model2.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base_rate = sum(train['target']) / len(train)\n",
    "# print(str(base_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(train[train_fields])\n",
    "truth = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexes = list(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotly Plotting Bit!\n",
    "\n",
    "# Create a trace\n",
    "trace_true = Scatter(\n",
    "    x = indexes[:500],\n",
    "    y = truth[:500],\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "trace_pred = Scatter(\n",
    "    x = indexes[:500],\n",
    "    y = prediction[:500],\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "data = [trace_true, trace_pred]\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deltas = truth - prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(deltas, 10, normed=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deltas.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
